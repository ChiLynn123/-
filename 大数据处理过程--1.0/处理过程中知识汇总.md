# 处理过程中知识汇总：

**背景：**接到临时需求，要处理3000w的数据，分为3000个txt文件，而每个文件1万域名。

**问题：**大量数据无法直接跑，太耗费时间，需首先清洗处理数据；

**思路：**筛选出有ip的网站进一步看剩下多少域名

1、服务器上32线程分别跑这3000个txt,生成3000个csv文件，里边保存四列数据

2、python pandas 进行数据分析

## 一、多线程编程知识

#### **思路：**

编程实现32线程跑2600个txt，先分list，每个list里边为32个str的数字，作为读取txt的路径以及生成的csv的文件名

```Python
#代码参考
import sys
sys.path.append(r'/usr/local/lib/python3.7/site-packages')
sys.path.append('/root/guangxi_test/1/con_log')
from qqwry import QQwry
from IPy import IP
import socket
import threading
import csv
import time
from con_log import logger_rule
import eventlet
eventlet.monkey_patch()
q = QQwry()
q.load_file('qqwry.dat')

def batch_query_and_print(path):
    """
    参数为一个str的数字，表示文件名
    输出为同名csv文件
    :param batch_list:
    :return:
    """
    with open(path+".csv", "a") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["country", "serve", "url", "ip"])
        with open(path+".txt") as f:
            url_list = f.read().splitlines()
            for url in url_list:
                ip =''
                address =[]
                try:
                    with eventlet.Timeout(1, False):
                        ip = socket.gethostbyname(url)
                        address = list(q.lookup(ip))
                    address.append(url)
                    address.append(ip)
                    writer.writerow(address)
                    logger_rule.info('{}  ip success'.format(url))
                except Exception as e:
                    #logger_rule.error()
                    #print('{}  ip fail,{}'.format(url, e))
                    pass

                #logger_rule.info()

def batch_query(batch_list):
    """
    参数为一个list，32个线程
    输出为32线程的结果
    :param batch_list:
    :return:
    """
    threads = []
    for batch in batch_list:
        logger_rule.info('********************')
        logger_rule.info('{} pian '.format(batch))
        t = threading.Thread(target=batch_query_and_print, args=(batch,))
        threads.append(t)
    for t in threads:
        t.setDaemon(True)
        t.start()
    for t in threads:
        t.join()

def threads_clm(thread_num):
    per_num = int(128/thread_num)
    for i in range(per_num):
        batch_list = []
        for j in range(thread_num):
            batch_list.append(str(32*i+(j+1)))
        a = time.time()
        batch_query(batch_list)
        #第几片成功
        logger_rule.info('{}  batch is  success'.format(i))
        b = time.time()
        print('shijain:***********{}**********'.format((b - a)))

if __name__ == "__main__":
    threads_clm(32)

```

#### **问题汇总：**

##### 1、参数传递问题：

```python
t = threading.Thread(target=worker,args=(i,))
#woker 为函数名
#args 为参数
```

第一个参数是线程函数变量，第二个参数args是一个数组变量参数，**如果只传递一个值，就只需要i, 如果需要传递多个参数，那么还可以继续传递下去其他的参数，其中的逗号不能少，元组中只包含一个元素时，需要在元素后面添加逗号**。

##### 2、多线程详解

https://blog.csdn.net/weixin_40481076/article/details/101594705?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.baidujs&dist_request_id=&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.baidujs

## 二、linux知识

#### 1、后台运行：

```python 
nohup  python  脚本.py  &
#日志、输出内容会到nohup.out
```

 相关知识详解： https://blog.csdn.net/weixin_42840933/article/details/85780125

#### 2、一些使用的命令

（1）将当前文件夹内的所有.csv结尾的文件移动到data/test/文件夹内：

```
cp -r ./*.csv*  /data/test/
```

（2）将当前目录下所有的txt文件名（包括后缀）提取到1.txt内：

```python 
ls -1 | grep ".txt$" > 1.txt
```

（3）删除当前文件夹内的所有的以.txt后缀的文件：

```python 
find . -name  "*.txt*"|xargs rm -rf 
```

（4）统计当前目录下的csv文件的个数：

```python 
find . -name "*.csv*"|wc -l
```

（5）杀死进程

```python 
kill -9 进程号
```

（6）将/home/wwwroot/xahot/ 这个目录下所有文件和文件夹打包为当前目录下的xahot.zip：

```python 
zip -q -r  xahot.zip  /home/wwwroot/xahot
```

解压当前目录下的test.zip文件：

```
unzip test.zip
```

###### 补充zip 和 unzip的知识：

**zip 压缩方法：**

压缩当前的文件夹 `zip -r ./xahot.zip ./* -r`表示递归
`zip [参数] [打包后的文件名] [打包的目录路径]`

**linux zip命令参数列表：**

​    -a 将文件转成ASCII模式
​    -F 尝试修复损坏的压缩文件
​    -h 显示帮助界面
​    -m 将文件压缩之后，删除源文件

​    -n 特定字符串 不压缩具有特定字尾字符串的文件
​    -o 将压缩文件内的所有文件的最新变动时间设为压缩时候的时间
​    -q 安静模式，在压缩的时候不显示指令的执行过程
​    -r 将指定的目录下的所有子目录以及文件一起处理
​    -S 包含系统文件和隐含文件（S是大写）
​    -t 日期 把压缩文件的最后修改日期设为指定的日期，日期格式为mmddyyyy

**举例：**

将/home/wwwroot/xahot/ 这个目录下所有文件和文件夹打包为当前目录下的xahot.zip

`zip –q –r xahot.zip /home/wwwroot/xahot`

上面的命令操作是将绝对地址的文件及文件夹进行压缩.以下给出压缩相对路径目录

比如目前在Bliux这个目录下,执行以下操作可以达到以上同样的效果.

`zip –q –r xahot.zip xahot`

比如现在我的xahot目录下,我操作的zip压缩命令是

`zip –q –r xahot.zip *`

以上是在安静模式下进行的，而且包含系统文件和隐含文件

**unzip语 法：**


unzip [-cflptuvz][-agCjLMnoqsVX][-P<密码>][.zip文件][文件][-d<目 录>][-x<文件>] 或 unzip [-Z]

补充说明：unzip为.zip压缩文件的解压缩程序。

**unzip参 数：**

​    -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。
​    -f 更新现有的文件。
​    -l 显示压缩文件内所包含的文件。
​    -p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。
​    -t 检查压缩文件是否正确。
​    -u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。
​    -v 执行是时显示详细的信息。
​    -z 仅显示压缩文件的备注文字。
​    -a 对文本文件进行必要的字符转换。
​    -b 不要对文本文件进行字符转换。
​    -C 压缩文件中的文件名称区分大小写。
​    -j 不处理压缩文件中原有的目录路径。
​    -L 将压缩文件中的全部文件名改为小写。
​    -M 将输出结果送到more程序处理。
​    -n 解压缩时不要覆盖原有的文件。
​    -o 不必先询问用户，unzip执行后覆盖原有文件。
​    -P<密码> 使用zip的密码选项。
​    -q 执行时不显示任何信息。
​    -s 将文件名中的空白字符转换为底线字符。
​    -V 保留VMS的文件版本信息。
​    -X 解压缩时同时回存文件原来的UID/GID。
​    [.zip文件] 指定.zip压缩文件。
​    [文件] 指定要处理.zip压缩文件中的哪些文件。
​    -d<目录> 指定文件解压缩后所要存储的目录。
​    -x<文件> 指定不要处理.zip压缩文件中的哪些文件。
​    -Z unzip -Z等于执行zipinfo指令

**举例：**

将/home/wwwroot/xahot.zip解压到当前目录

`unzip xahot.zip`

如果出现这个提示：
`-bash: zip: command not found` 

 不能执行ZIP压缩，是因为没有安装ZIP，
运行下这条安装命令即可  `yum install zip`

## 三、数据分析知识

**需求：**需要对多线程跑出来的3000个csv文件进行合并处理，每个csv文件里边有四个字段，含有大量NaN数据需要进行清洗。

```python
#-*- coding : utf-8 -*-
import pandas as pd
import os
import sys
filePath = 'F:/csv_he'
allxls=os.listdir(filePath) #提取filepath目录下的所有文件名，写入list中
allxlss = []
for i in allxls:
    j = ('F:/csv_he/'+i)
    allxlss.append(j)
   #所有的全路径list

data = pd.DataFrame(columns=['1', '2', '3', '4'])
for i in allxlss:
    #读文件
    df = pd.read_csv(i, encoding='utf8')
    #去除含有NaN的行
    df = df.dropna()
    #打印去空之后的数据行列数
    print('{} count is {}'.format(i, df.shape))
    #路径list下的文件全合并
    data = data.append(df)
#打印输出合并文件的大小
print('data count is {}'.format(data.shape))
#对特定列进行去重处理
data = data.drop_duplicates(['url'])
#打印输出去重后的数据大小
print('data drop_duplicates count is {}'.format(data.shape))
#保存为csv文件
data.to_csv('F:/data_sum.csv', encoding='utf-8')
```

#### 1、dataframe合并：

 https://blog.csdn.net/qq_41853758/article/details/83280104

#### 2、数据清洗：

https://zhuanlan.zhihu.com/p/32572237

#### 3、pandas读取csv文件：

https://www.jianshu.com/p/7ac36fafebea

#### 4、分类：

https://blog.csdn.net/waple_0820/article/details/80514073

#### 5、分类统计某一列：

`df['id'].value_counts()`

#### 6、根据条件新建列并赋值（匿名函数apply）：

`frame['panduan'] = frame.city.apply(lambda x: 1 if 'ing' in x else 0)`

#### 7、Python dataframe修改列顺序：

```python
order = ['date', 'time', 'open', 'high', 'low', 'close', 'volumefrom', 'volumeto']   

df = df[order]
```

## 四、遇到的其他问题

1、安装使用用于在qqwry.dat里查找IP地址归属地，执行此命令即可安装：`pip install qqwry-py3`

2、时间控制：

（1）采用eventlet，但是与gunicorn不兼容使用

```Python
import eventlet
q.load_file('qqwry.dat')
with eventlet.Timeout(1, False):
	********
```

（2）采用`socket.setdefaulttimeout(1)`

时间控制参考：https://www.jianshu.com/p/9c264dd8ba5d

2、进程和线程  https://www.cnblogs.com/ssyfj/p/9017383.html

3、学习编写Py程序时突然报错：

```python
SyntaxError: Non-UTF-8 code starting with '\xbb' in file C:\Users\v\Desktop\Test.py on line 4, but no encoding declared; see http://python.org/dev/peps/pep-0263/ for details
```

由于python内部使用的是unicode编码，而外部却要面对千奇百怪的各种编码，因此很容易导致编码问题
解决方法：在代码顶部加入

    # -*- coding: gbk -*-
